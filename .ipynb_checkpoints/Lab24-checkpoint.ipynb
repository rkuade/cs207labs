{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Crawlers\n",
    "\n",
    "Code for this lab is almost entirely taken and modified from Brent Slatkin's Pycon 2014 talk, since it provides a beautiful illustration of the entire process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Synchronous Blocking Crawler\n",
    "\n",
    "This code, taken from Brent's talk, is provided to you as an example of a synxhronous, single-threaded crawler you will make async"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from urllib.parse import urljoin\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import urlunparse\n",
    "import re\n",
    "import requests\n",
    "URL_EXPR = re.compile(\n",
    "    '([a-zA-Z]+\\s*=\\s*[\"\\'])'   # Tag attribute: href=\"\n",
    "    '(?P<url>'\n",
    "        '((http(s?):)?'         # Optional scheme\n",
    "        '//[^\"\\'\\s\\\\\\\\</]+)?'   # Optional domain\n",
    "        '/[^\"\\'\\s\\\\\\\\<]*'       # Required path\n",
    "    ')')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def canonicalize(url):\n",
    "    parts = list(urlparse(url))\n",
    "    if parts[2] == '':\n",
    "        parts[2] = '/'  # Empty path equals root path\n",
    "    parts[5] = ''       # Erase fragment\n",
    "    return urlunparse(parts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Notice the quick and dirty use of assert's here to throw exceptions if something goes wrong. The calling code should catch generic exceptions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fetch(url):\n",
    "    print(\"Doing\", url)\n",
    "    response = requests.get(url)\n",
    "    assert response.status_code == 200\n",
    "    data = response.content#get as bytes\n",
    "    assert data\n",
    "    return data.decode('utf-8')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "fetch(\"http://www.xkcd.com/353\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For simplicity, we keep to the same site for now. You can pass over this code, it just extracts urls on the same domain from the page using regular expressions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def same_domain(a, b):\n",
    "    parsed_a = urlparse(a)\n",
    "    parsed_b = urlparse(b)\n",
    "    if parsed_a.netloc == parsed_b.netloc:\n",
    "        return True\n",
    "    if (parsed_a.netloc == '') ^ (parsed_b.netloc == ''):  # Relative paths\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def extract(url):\n",
    "    data = fetch(url)\n",
    "    found_urls = set()\n",
    "    for match in URL_EXPR.finditer(data):\n",
    "        found = canonicalize(match.group('url'))\n",
    "        if same_domain(url, found):\n",
    "            found_urls.add(urljoin(url, found))\n",
    "    return url, len(data), sorted(found_urls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "extract(\"http://www.xkcd.com/353\")[2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def extract_multi(to_fetch, seen_urls):\n",
    "    results = []\n",
    "    for url in to_fetch:\n",
    "        if url in seen_urls: \n",
    "            continue\n",
    "        seen_urls.add(url)\n",
    "        try:\n",
    "            results.append(extract(url))\n",
    "        except Exception:\n",
    "            continue\n",
    "    return results\n",
    "\n",
    "\n",
    "def crawl(start_url, max_depth=1):\n",
    "    seen_urls = set()\n",
    "    to_fetch = [canonicalize(start_url)]\n",
    "    results = []\n",
    "    for depth in range(max_depth + 1):\n",
    "        batch = extract_multi(to_fetch, seen_urls)\n",
    "        to_fetch = []\n",
    "        for url, datalen, found_urls in batch:\n",
    "            results.append((depth, url, datalen))\n",
    "            to_fetch.extend(found_urls)\n",
    "\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "cr = crawl(\"http://www.xkcd.com/353\")\n",
    "cr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Synchronous crawler, async style\n",
    "\n",
    "(using yield from)\n",
    "\n",
    "Just like in the lecture, let us slowly bring in the async technology, still keeping a synchronous crawler going. This means that we'll have one `yield from` after another.\n",
    "\n",
    "We write the fetcher async now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import asyncio, aiohttp\n",
    "\n",
    "@asyncio.coroutine\n",
    "def fetch_async(url):\n",
    "    print(\"Doing\", url)\n",
    "    response = yield from aiohttp.request('GET', url)\n",
    "    try:\n",
    "        assert response.status == 200\n",
    "        data = yield from response.read()\n",
    "        assert data\n",
    "        return data.decode('utf-8')\n",
    "    finally:\n",
    "        response.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write the extractor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@asyncio.coroutine\n",
    "def extract_async(url):\n",
    "    #your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We wrap the top level coroutine in a task. Since a task is a future, we can also get its result in this form."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future = asyncio.Task(extract_async('http://www.xkcd.com/353'))\n",
    "#future = extract_async('http://www.xkcd.com/353')\n",
    "#you could do the above but could not access the result as \n",
    "#future.result()\n",
    "\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(future)\n",
    "#loop.close() ONLY DO IF NOT IN REPL OR YOU WILL BE HOSED\n",
    "future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Write the multi-extractor and crawler\n",
    "\n",
    "Note that you are writing the multi-extractor using async syntax but the `yield from`s are serialized."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@asyncio.coroutine\n",
    "def extract_multi_async(to_fetch, seen_urls):\n",
    "    #your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "@asyncio.coroutine\n",
    "def crawl_async(start_url, max_depth=1):\n",
    "    #your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run the entire crawler now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future = asyncio.Task(crawl_async('http://www.xkcd.com/353', max_depth=1))\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(future)\n",
    "future.result()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  3. Asynchronous crawler with `async def` and `await`: Many simultaneous fetches\n",
    "\n",
    "Rewrite all the code here. You will need to make two changes:\n",
    "\n",
    "1. `yield from` -> `await`, decorator -> `async def`\n",
    "2. note that `extract_multi_async` upstairs was seriealized. Use futures from `asyncio.as_completed` to change this.\n",
    "\n",
    "The first two are just copied over"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "async def fetch_async(url):\n",
    "    #your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "async def extract_async(url):\n",
    "    #your code here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Surprisingly, one of these next two is unchanged except for the syntax. Which one? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "async def extract_multi_async(to_fetch, seen_urls):\n",
    "    #your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "async def crawl_async(start_url, max_depth=1):\n",
    "    #your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future = asyncio.Task(crawl_async('http://www.xkcd.com/353', max_depth=1))\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(future)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Concurrent Crawls\n",
    "\n",
    "We can even do concurrent crawls to multiple web sites. Implement this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "urls = ['http://www.xkcd.com/353', 'http://what-if.xkcd.com/148/']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "async def crawl_multi_async(urls):\n",
    "    #your code here\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "future = asyncio.Task(crawl_multi_async(urls))\n",
    "loop = asyncio.get_event_loop()\n",
    "loop.run_until_complete(future)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
